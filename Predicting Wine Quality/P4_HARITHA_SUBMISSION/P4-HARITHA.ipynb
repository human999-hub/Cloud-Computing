{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "430af331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder,LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb42ef64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting prettytable\n",
      "  Downloading prettytable-3.10.0-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from prettytable) (0.2.13)\n",
      "Downloading prettytable-3.10.0-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: prettytable\n",
      "Successfully installed prettytable-3.10.0\n"
     ]
    }
   ],
   "source": [
    "# Install prettytable library\n",
    "!pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12ca03fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "# Establish connection to AWS S3\n",
    "s3_client = boto3.client('s3',region_name='eu-west-3')\n",
    "bucket_name = 'p4-haritha'\n",
    "data_key = 'p4_haritha_wine_input_data/winequality-white.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13e8e0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve dataset from S3 bucket\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=data_key)\n",
    "data_content = response['Body'].read().decode('utf-8')\n",
    "df = pd.read_csv(StringIO(data_content), sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cae3e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=pd.read_csv('winequality-white.csv', delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac7c3f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to categorize wine quality\n",
    "def categorize_quality(quality):\n",
    "    if 0 <= quality <= 4:\n",
    "        return \"Low Quality\"\n",
    "    elif 5 <= quality <= 7:\n",
    "        return \"Average Quality\"\n",
    "    elif 8 <= quality <= 10:\n",
    "        return \"High Quality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "995ed58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume all features are continuous except the target 'quality'\n",
    "continuous_features = df.columns.difference(['quality']).tolist()\n",
    "\n",
    "# Transform 'quality' into categorical bins\n",
    "df['quality'] = df['quality'].apply(categorize_quality)\n",
    "#print(df.head())\n",
    "# Initialize LabelEncoder and encode the quality labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['quality_encoded']=label_encoder.fit_transform(df['quality'])\n",
    "# Define a pipeline for transforming the data\n",
    "pipeline = ColumnTransformer([\n",
    "    ('scaler', StandardScaler(), continuous_features)  # Apply standardization\n",
    "    # Add other transformers here if needed\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81452a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and target\n",
    "X = df.drop('quality', axis=1)\n",
    "y = df['quality_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "720ef529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validate, test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_validate, X_test, y_validate, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "X_train_transformed = pipeline.fit_transform(X_train)\n",
    "X_validate_transformed = pipeline.transform(X_validate)\n",
    "X_test_transformed = pipeline.transform(X_test)\n",
    "\n",
    "# Convert transformed arrays back to DataFrame\n",
    "X_train_transformed = pd.DataFrame(X_train_transformed, columns=continuous_features)\n",
    "X_validate_transformed = pd.DataFrame(X_validate_transformed, columns=continuous_features)\n",
    "X_test_transformed = pd.DataFrame(X_test_transformed, columns=continuous_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "206bb5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Metrics for Test Data:\n",
      "Accuracy: 0.96\n",
      "Precision: 0.83\n",
      "Recall: 0.63\n",
      "Random Forest Metrics for validate data:\n",
      "Accuracy: 0.95\n",
      "Precision: 0.93\n",
      "Recall: 0.51\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "\n",
    "# Train the models\n",
    "rf_model.fit(X_train_transformed, y_train)\n",
    "\n",
    "\n",
    "# Predictions\n",
    "rf_predictions = rf_model.predict(X_test_transformed)\n",
    "\n",
    "\n",
    "# Evaluate the models\n",
    "def evaluate_model(predictions, y_test):\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average='macro')\n",
    "    recall = recall_score(y_test, predictions, average='macro')\n",
    "    return accuracy, precision, recall\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rf_accuracy, rf_precision, rf_recall = evaluate_model(rf_predictions, y_test)\n",
    "\n",
    "\n",
    "print(\"Random Forest Metrics for Test Data:\")\n",
    "print(\"Accuracy: {:.2f}\".format(rf_accuracy))\n",
    "print(\"Precision: {:.2f}\".format(rf_precision))\n",
    "print(\"Recall: {:.2f}\".format(rf_recall))\n",
    "\n",
    "\n",
    "validate_prediction_rf=rf_model.predict(X_validate_transformed)\n",
    "v_rf_accuracy, v_rf_precision, v_rf_recall = evaluate_model(validate_prediction_rf, y_validate)\n",
    "print(\"Random Forest Metrics for validate data:\")\n",
    "print(\"Accuracy: {:.2f}\".format(v_rf_accuracy))\n",
    "print(\"Precision: {:.2f}\".format(v_rf_precision))\n",
    "print(\"Recall: {:.2f}\".format(v_rf_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfca364f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier Metrics for Test data:\n",
      "Accuracy: 0.92\n",
      "Precision: 0.58\n",
      "Recall: 0.66\n",
      "Decision Tree Classifier Metrics for validate data:\n",
      "Accuracy: 0.90\n",
      "Precision: 0.54\n",
      "Recall: 0.60\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# Decision Tree Classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train_transformed, y_train)\n",
    "dt_pred_test = dt_classifier.predict(X_test_transformed)\n",
    "\n",
    "\n",
    "# Evaluation metrics for Decision Tree\n",
    "test_acc_dt = round(accuracy_score(y_test, dt_pred_test), 2)\n",
    "test_prec_dt = round(precision_score(y_test, dt_pred_test,average ='macro'), 2)\n",
    "test_rec_dt = round(recall_score(y_test, dt_pred_test,average ='macro'), 2)\n",
    "val_pred_dt = dt_classifier.predict(X_validate_transformed)\n",
    "val_acc_dt = round(accuracy_score(y_validate, val_pred_dt), 2)\n",
    "val_prec_dt = round(precision_score(y_validate, val_pred_dt,average ='macro'), 2)\n",
    "val_rec_dt = round(recall_score(y_validate, val_pred_dt,average ='macro'), 2)\n",
    "\n",
    "\n",
    "print(\"Decision Tree Classifier Metrics for Test data:\")\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(test_acc_dt))\n",
    "print(\"Precision: {:.2f}\".format(test_prec_dt))\n",
    "print(\"Recall: {:.2f}\".format(test_rec_dt))\n",
    "\n",
    "\n",
    "print(\"Decision Tree Classifier Metrics for validate data:\")\n",
    "print(\"Accuracy: {:.2f}\".format(val_acc_dt))\n",
    "print(\"Precision: {:.2f}\".format(val_prec_dt))\n",
    "print(\"Recall: {:.2f}\".format(val_rec_dt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c23352d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary\n",
    "quality_mapping = {0: 'Average Quality', 1: 'High Quality', 2: 'Low Quality'}\n",
    "\n",
    "# Apply mapping to the array\n",
    "validate_prediction_mapped_rf= [quality_mapping[prediction] for prediction in validate_prediction_rf]\n",
    "\n",
    "# Create a DataFrame\n",
    "df_prediction_rf = pd.DataFrame(validate_prediction_mapped_rf, columns=['Predicted Quality'])\n",
    "\n",
    "# Save as CSV\n",
    "df_prediction_rf.to_csv('predicted_quality_rf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddab8c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply mapping to the array\n",
    "validate_prediction_mapped_dt= [quality_mapping[prediction] for prediction in val_pred_dt]\n",
    "\n",
    "# Create a DataFrame\n",
    "df_prediction_dt = pd.DataFrame(validate_prediction_mapped_dt, columns=['Predicted Quality'])\n",
    "\n",
    "# Save as CSV\n",
    "df_prediction_dt.to_csv('predicted_quality_dt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d172831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Metric for Random Forest Classifier:\n",
      "+--------------------+-------------+----------------+\n",
      "| Performance Metric | Testing Set | Validation Set |\n",
      "+--------------------+-------------+----------------+\n",
      "|      Accuracy      |     0.96    |      0.95      |\n",
      "|     Precision      |     0.83    |      0.93      |\n",
      "|       Recall       |     0.63    |      0.51      |\n",
      "+--------------------+-------------+----------------+\n",
      "Performance Metric for Decision Tree Classifier:\n",
      "+--------------------+-------------+----------------+\n",
      "| Performance Metric | Testing Set | Validation Set |\n",
      "+--------------------+-------------+----------------+\n",
      "|      Accuracy      |     0.92    |      0.9       |\n",
      "|     Precision      |     0.58    |      0.54      |\n",
      "|       Recall       |     0.66    |      0.6       |\n",
      "+--------------------+-------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "import numpy as np\n",
    "\n",
    "rf_table = PrettyTable()\n",
    "rf_table.field_names = [\"Performance Metric\", \"Testing Set\", \"Validation Set\"]\n",
    "rf_table.add_row([\"Accuracy\", np.round(rf_accuracy,2), np.round(v_rf_accuracy,2)])\n",
    "rf_table.add_row([\"Precision\", np.round(rf_precision,2), np.round(v_rf_precision,2)])\n",
    "rf_table.add_row([\"Recall\", np.round(rf_recall,2), np.round(v_rf_recall,2)])\n",
    "\n",
    "\n",
    "dt_metrics_table = PrettyTable()\n",
    "dt_metrics_table.field_names = [\"Performance Metric\", \"Testing Set\", \"Validation Set\"]\n",
    "dt_metrics_table.add_row([\"Accuracy\", test_acc_dt, val_acc_dt])\n",
    "dt_metrics_table.add_row([\"Precision\", test_prec_dt, val_prec_dt])\n",
    "dt_metrics_table.add_row([\"Recall\", test_rec_dt, val_rec_dt])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Performance Metric for Random Forest Classifier:\")\n",
    "print(rf_table)\n",
    "print(\"Performance Metric for Decision Tree Classifier:\")\n",
    "print(dt_metrics_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2b9d0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"p4_haritha_metrics_summary.txt\", \"w\") as file:\n",
    "    file.write(\"Performance Metrics for Random Forest Classifier:\\n\")\n",
    "    file.write(str(rf_table))\n",
    "    file.write(\"\\n\\nPerformance Metrics for Decision Tree Classifier:\\n\")\n",
    "    file.write(str(dt_metrics_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "134e5bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.client('s3',region_name = 'eu-west-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b142f9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics file uploaded successfully to the S3 Bucket with the specified file path: p4_haritha_output/p4_haritha_metrics_summary.txt\n"
     ]
    }
   ],
   "source": [
    "s3_bucket_p4_haritha = \"p4-haritha\"\n",
    "s3_folder_path = \"p4_haritha_output/p4_haritha_metrics_summary.txt\"\n",
    "s3.upload_file('p4_haritha_metrics_summary.txt', s3_bucket_p4_haritha, s3_folder_path)\n",
    "print(\"Metrics file uploaded successfully to the S3 Bucket with the specified file path:\", s3_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "714f295d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction results file uploaded successfully to the S3 Bucket with file path: p4_haritha_output/Random Forest Classifier Model/predicted_quality_rf.csv\n"
     ]
    }
   ],
   "source": [
    "s3_classifier_path = \"p4_haritha_output/Random Forest Classifier Model/predicted_quality_rf.csv\" \n",
    "s3.upload_file('predicted_quality_rf.csv', s3_bucket_p4_haritha, s3_classifier_path)\n",
    "print(\"Prediction results file uploaded successfully to the S3 Bucket with file path:\", s3_classifier_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d19d6893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction results file uploaded successfully to the S3 Bucket with file path: p4_haritha_output/Decision Tree Classifier Model/predicted_quality_dt.csv\n"
     ]
    }
   ],
   "source": [
    "s3_classifier_path = \"p4_haritha_output/Decision Tree Classifier Model/predicted_quality_dt.csv\" \n",
    "s3.upload_file('predicted_quality_dt.csv', s3_bucket_p4_haritha, s3_classifier_path)\n",
    "print(\"Prediction results file uploaded successfully to the S3 Bucket with file path:\", s3_classifier_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6230ded6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
